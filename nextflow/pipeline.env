# Pipeline configuration (shell-style KEY=VALUE). Edit these values for your run.
# Paths may be absolute or relative to the repository root.

# Which cohort directory contains tumour inputs. The script will look for <COHORT_DIR>/input/<tumour>/ALPACA_input_table.csv
COHORT_DIR="../ALPACA-model/tests/multi-tumour" # expected to contain 'input' subdirectory which in turn is expected to contain a subdirectory per tumour
ALPACA_WORK="alpaca-work" # for storing partial output
NFX_REPORTS="nextflow/reports"

# Run modalities
DEBUG=1

# Execution profile: local or slurm
ENV_PROFILE="local"

# Optional conda environment (name or YAML). If set, Nextflow will use conda for processes.
# Example: CONDA_ENV="/path/to/env.yaml" or CONDA_ENV="my-conda-env"
CONDA_ENV="/camp/lab/swantonc/working/piotrpawlik/conda/C/main"

# Path to repository root / script dir (used to resolve scripts/segment_worker.py)
SCRIPT_DIR="scripts"

# Worker/parallelism defaults
# on HPC, separate node will be requested per worker, and each worker we use the number of CPUs specified below
WORKERS=1
CPUS=1
SEGMENTS_PER_CLAIM=4

# Pool and intermediate directories (relative to repo root or absolute)
POOL_DIR="$ALPACA_WORK/pool"
IN_PROGRESS_DIR="$ALPACA_WORK/in_progress"
DONE_DIR="$ALPACA_WORK/done"
FAILED_DIR="$ALPACA_WORK/failed"
OUTPUTS_DIR="$ALPACA_WORK/outputs"

# Extra arguments passed to ALPACA via the worker (empty by default). Escape as a single string.
ALPACA_ARGS="--objectives D"
